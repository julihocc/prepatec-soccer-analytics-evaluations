# Ejercicio Semana 13: M√©tricas Avanzadas de Evaluaci√≥n

**Curso:** Introducci√≥n a Data Science con F√∫tbol  
**Bloque:** 3 - Mi Primera Predicci√≥n con Datos de F√∫tbol  
**Semana:** 13 - Evaluaci√≥n Avanzada de Modelos  
**Tiempo estimado:** 60 minutos  
**Tipo:** Individual  
**Puntuaci√≥n:** 100 puntos (25 puntos por parte)

## Objetivos de Aprendizaje

Al completar este ejercicio, el estudiante ser√° capaz de:

1. **Implementar** m√∫ltiples m√©tricas de evaluaci√≥n m√°s all√° de accuracy
2. **Interpretar** matrices de confusi√≥n y curvas ROC para modelos deportivos
3. **Evaluar** la robustez de modelos con validaci√≥n cruzada avanzada
4. **Generar** reportes profesionales de evaluaci√≥n de modelos

## Contexto del Problema

Trabajaremos como analistas para **Atl√©tico de Madrid**, evaluando la efectividad real de nuestros modelos predictivos. No basta con conocer el accuracy general; necesitamos entender exactamente qu√© tan bien funcionan nuestros modelos en diferentes situaciones para tomar decisiones estrat√©gicas fundamentadas.

---

# Ejercicio Integrador: Laboratorio de Evaluaci√≥n Atl√©tico de Madrid

## Parte 1: Implementaci√≥n de M√©tricas Avanzadas (25 puntos)

### Objetivo
Desarrollar un sistema completo de evaluaci√≥n con m√∫ltiples m√©tricas para modelos del Atl√©tico de Madrid.

### Instrucciones Detalladas

**Paso 1:** Configura laboratorio de evaluaci√≥n avanzada:

```python
# Configuraci√≥n del laboratorio de evaluaci√≥n Atl√©tico de Madrid
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score,
    roc_curve, precision_recall_curve, average_precision_score
)
from sklearn.model_selection import validation_curve, learning_curve
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n visual Atl√©tico de Madrid
colores_atletico = ['#CE2524', '#FFFFFF', '#223447', '#FFD700']
sns.set_theme(style="whitegrid", palette=colores_atletico)
plt.rcParams['figure.figsize'] = (14, 10)
plt.rcParams['font.size'] = 11

print("=== LABORATORIO EVALUACI√ìN ATL√âTICO DE MADRID ===")
print("Sistema avanzado de m√©tricas de evaluaci√≥n iniciado")
print("¬°An√°lisis exhaustivo de modelos en marcha!")
```

**Paso 2:** Genera dataset realista del Atl√©tico:

```python
# Dataset optimizado para Atl√©tico de Madrid

# TU C√ìDIGO AQU√ç:
# 1. Crear dataset de 500 partidos del Atl√©tico de Madrid
# 2. Incluir variables espec√≠ficas del estilo del Atl√©tico:
#    - solidez_defensiva: Rating defensivo (1-10)
#    - efectividad_contraataque: % goles en contraataque
#    - condicion_fisica: Estado f√≠sico del equipo (1-10)
#    - intensidad_partido: Nivel de intensidad esperado (1-10)
#    - experiencia_plantilla: A√±os promedio experiencia
#    - motivacion_extra: Factor motivacional especial (0-1)
# 3. Variable objetivo: 'victoria_convincente' (1 si gan√≥ sin sufrir, 0 si no)
# 4. Crear distribuci√≥n realista (35% victorias convincentes)

np.random.seed(456)  # Para reproducibilidad

# Generar datos del Atl√©tico
partidos_atletico = []
for i in range(500):
    # Variables caracter√≠sticas del Atl√©tico
    solidez_defensiva = np.random.normal(8.5, 1.2)
    efectividad_contraataque = np.random.normal(45, 12)
    condicion_fisica = np.random.normal(8.0, 1.0)
    intensidad_partido = np.random.normal(7.5, 1.5)
    experiencia_plantilla = np.random.normal(6.2, 2.0)
    motivacion_extra = np.random.beta(2, 5)  # Sesgado hacia valores bajos
    
    # Probabilidad de victoria convincente
    prob_victoria = (
        solidez_defensiva * 0.15 + 
        efectividad_contraataque * 0.01 + 
        condicion_fisica * 0.12 + 
        intensidad_partido * 0.08 + 
        experiencia_plantilla * 0.1 + 
        motivacion_extra * 1.5
    ) / 10
    
    # Ajustar para tener ~35% de victorias convincentes
    prob_victoria = max(0, min(1, prob_victoria * 0.8))
    
    partido = {
        'solidez_defensiva': solidez_defensiva,
        'efectividad_contraataque': efectividad_contraataque,
        'condicion_fisica': condicion_fisica,
        'intensidad_partido': intensidad_partido,
        'experiencia_plantilla': experiencia_plantilla,
        'motivacion_extra': motivacion_extra,
        'victoria_convincente': 1 if np.random.random() < prob_victoria else 0
    }
    partidos_atletico.append(partido)

df_atletico = pd.DataFrame(partidos_atletico)

print("=== DATASET ATL√âTICO GENERADO ===")
print(f"Total partidos: {len(df_atletico)}")
print(f"Distribuci√≥n victorias convincentes: {df_atletico['victoria_convincente'].value_counts()}")
print(f"Porcentaje victorias convincentes: {df_atletico['victoria_convincente'].mean():.1%}")
```

### Criterios de Evaluaci√≥n
- **Configuraci√≥n correcta del entorno de evaluaci√≥n** (10 puntos)
- **Dataset realista con variables espec√≠ficas del Atl√©tico** (15 puntos)

---

## Parte 2: An√°lisis Detallado con M√∫ltiples M√©tricas (25 puntos)

### Objetivo
Implementar y comparar m√∫ltiples m√©tricas de evaluaci√≥n para entender mejor el rendimiento de modelos.

### Instrucciones Detalladas

**Paso 3:** Entrena modelos y calcula m√©tricas completas:

```python
# Sistema completo de evaluaci√≥n de modelos

# TU C√ìDIGO AQU√ç:
# 1. Preparar datos y divisi√≥n train/test
# 2. Entrenar 3 modelos diferentes
# 3. Calcular accuracy, precision, recall, F1-score para cada modelo
# 4. Generar classification reports detallados
# 5. Crear matrices de confusi√≥n informativas

# Preparar datos
features = ['solidez_defensiva', 'efectividad_contraataque', 'condicion_fisica', 
           'intensidad_partido', 'experiencia_plantilla', 'motivacion_extra']
X = df_atletico[features]
y = df_atletico['victoria_convincente']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Modelos para evaluar
modelos = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'Regresi√≥n Log√≠stica': LogisticRegression(random_state=42)
}

# Diccionario para almacenar resultados
evaluaciones = {}

for nombre, modelo in modelos.items():
    # Entrenar modelo
    modelo.fit(X_train, y_train)
    
    # Predicciones
    y_pred = modelo.predict(X_test)
    y_proba = modelo.predict_proba(X_test)[:, 1]
    
    # Calcular m√©tricas
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_proba)
    
    evaluaciones[nombre] = {
        'modelo': modelo,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc,
        'y_pred': y_pred,
        'y_proba': y_proba
    }
    
    print(f"\n=== {nombre.upper()} ===")
    print(f"Accuracy: {accuracy:.3f}")
    print(f"Precision: {precision:.3f}")
    print(f"Recall: {recall:.3f}")
    print(f"F1-Score: {f1:.3f}")
    print(f"AUC: {auc:.3f}")

print("\n=== EVALUACI√ìN COMPLETA FINALIZADA ===")
```

**Paso 4:** Visualiza m√©tricas comparativas:

```python
# Visualizaci√≥n comparativa de m√©tricas

# TU C√ìDIGO AQU√ç:
# 1. Crear gr√°fico de barras con m√∫ltiples m√©tricas
# 2. Generar matrices de confusi√≥n para cada modelo
# 3. Crear tabla resumen de m√©tricas
# 4. Identificar fortalezas/debilidades de cada modelo
# 5. Interpretar qu√© significa cada m√©trica para el Atl√©tico

# DataFrame para comparaci√≥n
metricas_df = pd.DataFrame.from_dict(
    {nombre: {metrica: datos[metrica] for metrica in ['accuracy', 'precision', 'recall', 'f1', 'auc']}
     for nombre, datos in evaluaciones.items()}, 
    orient='index'
)

# Visualizaci√≥n comparativa
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Gr√°fico de barras m√∫ltiples
ax1 = axes[0, 0]
metricas_df.plot(kind='bar', ax=ax1, color=colores_atletico[:len(metricas_df.columns)])
ax1.set_title('Comparaci√≥n de M√©tricas: Atl√©tico de Madrid', fontsize=14, fontweight='bold')
ax1.set_ylabel('Puntuaci√≥n')
ax1.legend(title='M√©tricas')
ax1.tick_params(axis='x', rotation=45)

# Matrices de confusi√≥n
for i, (nombre, datos) in enumerate(list(evaluaciones.items())[:3]):
    row = (i + 1) // 2
    col = (i + 1) % 2
    if i < 2:
        ax = axes[row, col]
        cm = confusion_matrix(y_test, datos['y_pred'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', ax=ax,
                    xticklabels=['No Convincente', 'Convincente'],
                    yticklabels=['No Convincente', 'Convincente'])
        ax.set_title(f'Matriz Confusi√≥n: {nombre}')
        ax.set_xlabel('Predicci√≥n')
        ax.set_ylabel('Real')

plt.tight_layout()
plt.show()

# Tabla resumen
print("\n=== RESUMEN M√âTRICAS ATL√âTICO DE MADRID ===")
print(metricas_df.round(3))

# Interpretaci√≥n
mejor_accuracy = metricas_df['accuracy'].idxmax()
mejor_precision = metricas_df['precision'].idxmax()
mejor_recall = metricas_df['recall'].idxmax()
mejor_f1 = metricas_df['f1'].idxmax()

print(f"\nüìä MEJORES MODELOS POR M√âTRICA:")
print(f"‚Ä¢ Accuracy: {mejor_accuracy} ({metricas_df.loc[mejor_accuracy, 'accuracy']:.3f})")
print(f"‚Ä¢ Precision: {mejor_precision} ({metricas_df.loc[mejor_precision, 'precision']:.3f})")
print(f"‚Ä¢ Recall: {mejor_recall} ({metricas_df.loc[mejor_recall, 'recall']:.3f})")
print(f"‚Ä¢ F1-Score: {mejor_f1} ({metricas_df.loc[mejor_f1, 'f1']:.3f})")
```

### Criterios de Evaluaci√≥n
- **Implementaci√≥n correcta de m√∫ltiples m√©tricas** (15 puntos)
- **Visualizaciones informativas y an√°lisis comparativo** (10 puntos)

---

## Parte 3: Validaci√≥n Robusta y Curvas de Rendimiento (25 puntos)

### Objetivo
Implementar t√©cnicas avanzadas de validaci√≥n para asegurar que los modelos son robustos y confiables.

### Instrucciones Detalladas

**Paso 5:** Implementa validaci√≥n cruzada avanzada:

```python
# Validaci√≥n cruzada robusta

# TU C√ìDIGO AQU√ç:
# 1. Implementar validaci√≥n cruzada estratificada con k=5
# 2. Calcular m√∫ltiples m√©tricas para cada fold
# 3. Analizar variabilidad del rendimiento
# 4. Identificar modelos m√°s estables
# 5. Calcular intervalos de confianza

from sklearn.model_selection import cross_validate

# M√©tricas para validaci√≥n cruzada
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

# Validaci√≥n cruzada para cada modelo
cv_resultados = {}
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for nombre, modelo in modelos.items():
    scores = cross_validate(modelo, X, y, cv=kfold, scoring=scoring, return_train_score=True)
    
    cv_resultados[nombre] = {}
    for metrica in scoring:
        test_scores = scores[f'test_{metrica}']
        train_scores = scores[f'train_{metrica}']
        
        cv_resultados[nombre][metrica] = {
            'test_mean': test_scores.mean(),
            'test_std': test_scores.std(),
            'train_mean': train_scores.mean(),
            'train_std': train_scores.std(),
            'overfitting': train_scores.mean() - test_scores.mean()
        }
    
    print(f"\n=== VALIDACI√ìN CRUZADA: {nombre.upper()} ===")
    for metrica in scoring:
        resultado = cv_resultados[nombre][metrica]
        print(f"{metrica.upper()}:")
        print(f"  Test: {resultado['test_mean']:.3f} (¬±{resultado['test_std']:.3f})")
        print(f"  Train: {resultado['train_mean']:.3f} (¬±{resultado['train_std']:.3f})")
        print(f"  Overfitting: {resultado['overfitting']:.3f}")

print("\n=== VALIDACI√ìN CRUZADA COMPLETADA ===")
```

**Paso 6:** Genera curvas de rendimiento ROC y Precision-Recall:

```python
# Curvas de rendimiento avanzadas

# TU C√ìDIGO AQU√ç:
# 1. Generar curvas ROC para cada modelo
# 2. Crear curvas Precision-Recall
# 3. Calcular AUC para ambas curvas
# 4. Interpretar qu√© modelo es mejor seg√∫n cada curva
# 5. Analizar trade-offs entre precision y recall

plt.figure(figsize=(15, 5))

# Subplot 1: Curvas ROC
plt.subplot(1, 3, 1)
for nombre, datos in evaluaciones.items():
    fpr, tpr, _ = roc_curve(y_test, datos['y_proba'])
    auc = datos['auc']
    plt.plot(fpr, tpr, label=f'{nombre} (AUC = {auc:.3f})', linewidth=2)

plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Curvas ROC: Atl√©tico de Madrid')
plt.legend()
plt.grid(True, alpha=0.3)

# Subplot 2: Curvas Precision-Recall
plt.subplot(1, 3, 2)
for nombre, datos in evaluaciones.items():
    precision_curve, recall_curve, _ = precision_recall_curve(y_test, datos['y_proba'])
    avg_precision = average_precision_score(y_test, datos['y_proba'])
    plt.plot(recall_curve, precision_curve, label=f'{nombre} (AP = {avg_precision:.3f})', linewidth=2)

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Curvas Precision-Recall')
plt.legend()
plt.grid(True, alpha=0.3)

# Subplot 3: Estabilidad de modelos
plt.subplot(1, 3, 3)
modelos_nombres = list(cv_resultados.keys())
f1_means = [cv_resultados[nombre]['f1']['test_mean'] for nombre in modelos_nombres]
f1_stds = [cv_resultados[nombre]['f1']['test_std'] for nombre in modelos_nombres]

bars = plt.bar(modelos_nombres, f1_means, yerr=f1_stds, capsize=5, 
               color=colores_atletico[:len(modelos_nombres)], alpha=0.7)
plt.title('Estabilidad F1-Score (CV)')
plt.ylabel('F1-Score')
plt.xticks(rotation=45)

# A√±adir valores en barras
for bar, mean in zip(bars, f1_means):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{mean:.3f}', ha='center', fontweight='bold')

plt.tight_layout()
plt.show()

# An√°lisis de estabilidad
print("\n=== AN√ÅLISIS DE ESTABILIDAD ===")
for nombre in modelos_nombres:
    f1_data = cv_resultados[nombre]['f1']
    overfitting = f1_data['overfitting']
    variabilidad = f1_data['test_std']
    
    estabilidad = "Estable" if variabilidad < 0.05 else "Inestable"
    sobreajuste = "Alto" if overfitting > 0.1 else "Bajo" if overfitting < 0.05 else "Moderado"
    
    print(f"{nombre}:")
    print(f"  Variabilidad: {variabilidad:.3f} ({estabilidad})")
    print(f"  Overfitting: {overfitting:.3f} ({sobreajuste})")
```

### Criterios de Evaluaci√≥n
- **Implementaci√≥n correcta de validaci√≥n cruzada** (15 puntos)
- **Generaci√≥n e interpretaci√≥n de curvas ROC y PR** (10 puntos)

---

## Parte 4: Reporte Ejecutivo de Evaluaci√≥n (25 puntos)

### Objetivo
Crear un reporte profesional que comunique efectivamente la calidad y confiabilidad de los modelos.

### Instrucciones Detalladas

**Paso 7:** Identifica el modelo √≥ptimo considerando m√∫ltiples criterios:

```python
# Selecci√≥n del modelo √≥ptimo

# TU C√ìDIGO AQU√ç:
# 1. Crear sistema de scoring multi-criterio
# 2. Ponderar diferentes aspectos (accuracy, estabilidad, interpretabilidad)
# 3. Calcular score final para cada modelo
# 4. Justificar selecci√≥n del modelo recomendado
# 5. Analizar pros/contras de cada modelo

# Sistema de scoring ponderado
pesos = {
    'accuracy': 0.25,
    'precision': 0.20,
    'recall': 0.20,
    'f1': 0.20,
    'estabilidad': 0.15  # Inverso de la desviaci√≥n est√°ndar
}

# Calcular scores finales
scores_finales = {}
for nombre in modelos_nombres:
    score = 0
    
    # M√©tricas de rendimiento
    for metrica in ['accuracy', 'precision', 'recall', 'f1']:
        valor = cv_resultados[nombre][metrica]['test_mean']
        score += pesos[metrica] * valor
    
    # Estabilidad (inverso de desviaci√≥n est√°ndar)
    estabilidad = 1 - min(1, cv_resultados[nombre]['f1']['test_std'] * 2)
    score += pesos['estabilidad'] * estabilidad
    
    scores_finales[nombre] = score

# Ranking de modelos
ranking = sorted(scores_finales.items(), key=lambda x: x[1], reverse=True)

print("=== RANKING FINAL DE MODELOS ===")
for i, (nombre, score) in enumerate(ranking, 1):
    print(f"{i}. {nombre}: {score:.3f}")

modelo_recomendado = ranking[0][0]
print(f"\nüèÜ MODELO RECOMENDADO: {modelo_recomendado}")
```

**Paso 8:** Genera reporte ejecutivo completo:

```python
# Reporte ejecutivo final para Atl√©tico de Madrid

# TU C√ìDIGO AQU√ç:
# 1. Resumir metodolog√≠a de evaluaci√≥n
# 2. Presentar hallazgos clave sobre cada modelo
# 3. Recomendar modelo final con justificaci√≥n
# 4. Incluir limitaciones y pr√≥ximos pasos
# 5. Proporcionar gu√≠as de implementaci√≥n

print("=" * 80)
print("ATL√âTICO DE MADRID - REPORTE EVALUACI√ìN DE MODELOS")
print("An√°lisis Exhaustivo de Calidad y Confiabilidad Predictiva")
print("=" * 80)

print("\nüìã METODOLOG√çA DE EVALUACI√ìN:")
print("‚Ä¢ Validaci√≥n cruzada estratificada (5 folds)")
print("‚Ä¢ M√∫ltiples m√©tricas: Accuracy, Precision, Recall, F1, AUC")
print("‚Ä¢ An√°lisis de estabilidad y overfitting")
print("‚Ä¢ Curvas ROC y Precision-Recall")
print("‚Ä¢ Sistema de scoring multi-criterio")

print(f"\nüèÜ MODELO RECOMENDADO: {modelo_recomendado}")
modelo_data = cv_resultados[modelo_recomendado]
print(f"‚Ä¢ F1-Score: {modelo_data['f1']['test_mean']:.3f} (¬±{modelo_data['f1']['test_std']:.3f})")
print(f"‚Ä¢ Precision: {modelo_data['precision']['test_mean']:.3f}")
print(f"‚Ä¢ Recall: {modelo_data['recall']['test_mean']:.3f}")
print(f"‚Ä¢ Score Final: {scores_finales[modelo_recomendado]:.3f}")

print("\nüìä AN√ÅLISIS POR MODELO:")
for nombre in modelos_nombres:
    data = cv_resultados[nombre]
    print(f"\n{nombre.upper()}:")
    print(f"  ‚úì Fortalezas:")
    
    # Identificar fortalezas
    if data['precision']['test_mean'] > 0.7:
        print(f"    - Alta precisi√≥n ({data['precision']['test_mean']:.3f})")
    if data['recall']['test_mean'] > 0.7:
        print(f"    - Alto recall ({data['recall']['test_mean']:.3f})")
    if data['f1']['test_std'] < 0.05:
        print(f"    - Muy estable (std: {data['f1']['test_std']:.3f})")
    
    print(f"  ‚ö† Consideraciones:")
    if data['f1']['overfitting'] > 0.1:
        print(f"    - Posible overfitting ({data['f1']['overfitting']:.3f})")
    if data['f1']['test_std'] > 0.08:
        print(f"    - Variabilidad alta ({data['f1']['test_std']:.3f})")

print("\n‚öΩ INSIGHTS PARA EL ATL√âTICO:")
print("1. La solidez defensiva es crucial para victorias convincentes")
print("2. Los contraataques efectivos aumentan significativamente las probabilidades")
print("3. La experiencia de la plantilla es un factor diferencial")
print("4. La motivaci√≥n extra puede ser determinante en partidos clave")

print("\nüöÄ RECOMENDACIONES DE IMPLEMENTACI√ìN:")
print("1. Usar el modelo recomendado para decisiones estrat√©gicas principales")
print("2. Monitorear m√©tricas de rendimiento semanalmente")
print("3. Reentrenar modelo mensualmente con nuevos datos")
print("4. Establecer umbrales de alerta para degradaci√≥n del rendimiento")

print("\n‚öôÔ∏è LIMITACIONES Y PR√ìXIMOS PASOS:")
print("‚Ä¢ Limitaci√≥n: Dataset simulado, necesita datos reales")
print("‚Ä¢ Pr√≥ximo: Incorporar datos de jugadores individuales")
print("‚Ä¢ Pr√≥ximo: A√±adir variables contextuales (rival, competici√≥n)")
print("‚Ä¢ Pr√≥ximo: Desarrollar modelos espec√≠ficos por tipo de partido")

# Dashboard final
plt.figure(figsize=(16, 10))

# Comparaci√≥n final de m√©tricas
plt.subplot(2, 3, 1)
for metrica in ['accuracy', 'precision', 'recall', 'f1']:
    valores = [cv_resultados[nombre][metrica]['test_mean'] for nombre in modelos_nombres]
    plt.plot(modelos_nombres, valores, marker='o', label=metrica, linewidth=2)
plt.title('Comparaci√≥n de M√©tricas')
plt.xticks(rotation=45)
plt.legend()
plt.grid(True, alpha=0.3)

# Estabilidad
plt.subplot(2, 3, 2)
estabilidades = [1 - cv_resultados[nombre]['f1']['test_std'] for nombre in modelos_nombres]
bars = plt.bar(modelos_nombres, estabilidades, color=colores_atletico[:len(modelos_nombres)])
plt.title('Estabilidad de Modelos')
plt.ylabel('Estabilidad (1 - std)')
plt.xticks(rotation=45)

# Score final
plt.subplot(2, 3, 3)
scores = list(scores_finales.values())
bars = plt.bar(modelos_nombres, scores, color=colores_atletico[:len(modelos_nombres)])
plt.title('Score Final Multi-Criterio')
plt.ylabel('Score Ponderado')
plt.xticks(rotation=45)

# Overfitting analysis
plt.subplot(2, 3, 4)
overfit_values = [cv_resultados[nombre]['f1']['overfitting'] for nombre in modelos_nombres]
bars = plt.bar(modelos_nombres, overfit_values, color='red', alpha=0.6)
plt.axhline(y=0.05, color='green', linestyle='--', label='Umbral Aceptable')
plt.title('An√°lisis de Overfitting')
plt.ylabel('Train - Test F1')
plt.xticks(rotation=45)
plt.legend()

# Matriz de confusi√≥n del mejor modelo
plt.subplot(2, 3, 5)
mejor_pred = evaluaciones[modelo_recomendado]['y_pred']
cm = confusion_matrix(y_test, mejor_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds',
            xticklabels=['No Convincente', 'Convincente'],
            yticklabels=['No Convincente', 'Convincente'])
plt.title(f'Matriz Confusi√≥n: {modelo_recomendado}')

# Distribuci√≥n de predicciones
plt.subplot(2, 3, 6)
mejor_proba = evaluaciones[modelo_recomendado]['y_proba']
plt.hist(mejor_proba[y_test == 0], alpha=0.5, label='No Convincente', bins=20)
plt.hist(mejor_proba[y_test == 1], alpha=0.5, label='Convincente', bins=20)
plt.title('Distribuci√≥n de Probabilidades')
plt.xlabel('Probabilidad Predicha')
plt.ylabel('Frecuencia')
plt.legend()

plt.suptitle('Dashboard Evaluaci√≥n Completa: Atl√©tico de Madrid', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()
```

### Criterios de Evaluaci√≥n
- **Sistema de scoring multi-criterio robusto** (15 puntos)
- **Reporte ejecutivo completo y profesional** (10 puntos)

## Criterios de Evaluaci√≥n General

### Correctitud T√©cnica (40 puntos)
- Implementaci√≥n correcta de m√∫ltiples m√©tricas de evaluaci√≥n
- Validaci√≥n cruzada apropiada y an√°lisis de estabilidad
- Generaci√≥n correcta de curvas ROC y Precision-Recall
- Sistema de scoring multi-criterio bien fundamentado

### Aplicaci√≥n Pr√°ctica (30 puntos)
- Interpretaci√≥n correcta de m√©tricas en contexto deportivo
- Identificaci√≥n apropiada de fortalezas y debilidades de modelos
- Recomendaciones pr√°cticas para implementaci√≥n
- An√°lisis realista de limitaciones y pr√≥ximos pasos

### Claridad y Documentaci√≥n (30 puntos)
- Explicaciones claras de cada m√©trica y su importancia
- Visualizaciones informativas y bien dise√±adas
- Reporte ejecutivo profesional y comprensible
- C√≥digo bien estructurado y comentado

## Instrucciones de Entrega

1. **Implementa todas las m√©tricas** de evaluaci√≥n correctamente
2. **Incluye an√°lisis comparativo** detallado entre modelos
3. **Verifica funcionamiento** de validaci√≥n cruzada y curvas
4. **Guarda como:** `[matricula]-ejercicio-semana-13.ipynb`
5. **Entrega antes del final de Semana 13**

## Recursos de Apoyo

- Notebook de la Semana 13: `metricas-avanzadas-evaluacion.ipynb`
- Documentaci√≥n scikit-learn: M√©tricas de evaluaci√≥n
- Gu√≠a de interpretaci√≥n de curvas ROC y Precision-Recall

---

**¬°Convierte al Atl√©tico de Madrid en el equipo con la evaluaci√≥n de modelos m√°s rigurosa del f√∫tbol mundial!** ‚öΩüìä

sns.set_theme(style="whitegrid", palette="viridis")
```

**1.2 Creaci√≥n de conjunto de datos desbalanceado**

- Crea un dataset donde las victorias locales sean 70% de los casos (m√°s realista)
- Incluye al menos 1000 partidos con variables relevantes
- Entrena tu mejor modelo de la semana anterior

**1.3 C√°lculo de m√©tricas b√°sicas**

Calcula y explica cada m√©trica:

- **Accuracy:** ¬øQu√© porcentaje de predicciones fueron correctas?
- **Precision:** De todas las victorias locales que predije, ¬øcu√°ntas fueron correctas?
- **Recall (Sensitivity):** De todas las victorias locales reales, ¬øcu√°ntas detect√©?
- **Specificity:** De todas las derrotas locales reales, ¬øcu√°ntas detect√©?
- **F1-Score:** Promedio arm√≥nico de precision y recall

**1.4 Interpretaci√≥n en contexto deportivo**

Para cada m√©trica, explica:

- ¬øQu√© significa en t√©rminos de predicciones deportivas?
- ¬øCu√°ndo ser√≠a m√°s importante cada una?
- ¬øQu√© m√©trica usar√≠as para un sitio de apuestas vs un entrenador?

### Entregables

- Dataset desbalanceado creado
- C√°lculo correcto de todas las m√©tricas
- Interpretaci√≥n detallada de cada m√©trica en contexto deportivo
- Recomendaciones sobre cu√°ndo usar cada m√©trica

---

## Ejercicio 2: Matrices de Confusi√≥n - An√°lisis Profundo (1 hora)

### Contexto

La matriz de confusi√≥n es como un "informe detallado" de d√≥nde se equivoca tu modelo. Nos permite identificar patrones espec√≠ficos de errores y fortalezas.

### Tareas

**2.1 Matrices de confusi√≥n visuales**

```python
from sklearn.metrics import ConfusionMatrixDisplay

# Crea matrices de confusi√≥n visuales para tus mejores 3 modelos
# Compara lado a lado
```

**2.2 An√°lisis detallado de errores**

Para cada modelo:

- **Verdaderos Positivos:** Victorias locales predichas correctamente
- **Falsos Positivos:** Predije victoria local pero fue derrota (Error Tipo I)
- **Falsos Negativos:** Predije derrota local pero fue victoria (Error Tipo II)
- **Verdaderos Negativos:** Derrotas locales predichas correctamente

**2.3 An√°lisis de casos espec√≠ficos**

- Identifica 10 casos de Falsos Positivos: ¬øQu√© tienen en com√∫n?
- Identifica 10 casos de Falsos Negativos: ¬øPor qu√© el modelo fall√≥?
- ¬øHay patrones identificables en los errores?

**2.4 Matriz de confusi√≥n normalizada**

- Crea matrices normalizadas por fila (recall) y por columna (precision)
- ¬øQu√© insights adicionales proporciona la normalizaci√≥n?
- ¬øTu modelo es mejor prediciendo victorias o derrotas?

**2.5 Costos de errores**

Considera diferentes escenarios:

- **Para apostadores:** ¬øQu√© error es m√°s costoso?
- **Para entrenadores:** ¬øQu√© error es m√°s problem√°tico?
- **Para medios deportivos:** ¬øQu√© error es m√°s embarazoso?

### Entregables

- Matrices de confusi√≥n visuales comparativas
- An√°lisis detallado de tipos de errores
- Identificaci√≥n de patrones en errores espec√≠ficos
- An√°lisis de costos relativos de diferentes tipos de errores

---

## Ejercicio 3: Curvas ROC y An√°lisis de Probabilidades (1 hora)

### Contexto

Las curvas ROC nos ayudan a entender el trade-off entre detectar victorias verdaderas y evitar falsas alarmas. Tambi√©n nos permiten ajustar el "umbral de decisi√≥n" de nuestro modelo.

### Tareas

**3.1 Curvas ROC comparativas**

```python
# Calcula y grafica curvas ROC para tus mejores modelos
from sklearn.metrics import roc_curve, auc

# Tu c√≥digo aqu√≠ para crear curvas ROC comparativas
```

**3.2 Interpretaci√≥n de AUC (Area Under Curve)**

- Calcula el AUC para cada modelo
- ¬øQu√© significa un AUC de 0.85 vs 0.92 en t√©rminos pr√°cticos?
- ¬øCu√°l es el modelo m√°s "discriminativo"?

**3.3 Ajuste de umbrales**

- Por defecto, el umbral es 0.5 (si probabilidad > 0.5, predice victoria)
- Experimenta con umbrales de 0.3, 0.5, 0.7
- ¬øC√≥mo cambian precision, recall y F1 con cada umbral?

**3.4 Curva Precision-Recall**

- Crea curvas Precision-Recall para datos desbalanceados
- ¬øQu√© informaci√≥n adicional proporciona vs curva ROC?
- ¬øCu√°l es m√°s informativa para tu problema espec√≠fico?

**3.5 Selecci√≥n de umbral √≥ptimo**

- Encuentra el umbral que maximiza F1-score
- Encuentra el umbral que balanc√©a precision y recall
- ¬øQu√© umbral recomendar√≠as para diferentes aplicaciones?

### Entregables

- Curvas ROC comparativas con AUC calculado
- An√°lisis de diferentes umbrales de decisi√≥n
- Curvas Precision-Recall para datos desbalanceados
- Recomendaciones de umbrales √≥ptimos para diferentes escenarios

---

## Ejercicio 4: Validaci√≥n Robusta y Confianza Estad√≠stica (1 hora)

### Contexto

¬øC√≥mo sabemos si nuestro modelo realmente funciona o solo tuvimos suerte con la divisi√≥n de datos? Necesitamos t√©cnicas de validaci√≥n que nos den confianza estad√≠stica en nuestros resultados.

### Tareas

**4.1 Validaci√≥n cruzada k-fold**

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Implementa validaci√≥n cruzada de 10 folds
cv_scores = cross_val_score(modelo, X, y, cv=10, scoring='accuracy')
```

**4.2 M√∫ltiples m√©tricas con validaci√≥n cruzada**

- Valida accuracy, precision, recall, f1 y AUC con 10-fold CV
- Calcula media y desviaci√≥n est√°ndar de cada m√©trica
- ¬øQu√© tan consistente es el rendimiento de tu modelo?

**4.3 Intervalos de confianza**

- Calcula intervalos de confianza del 95% para cada m√©trica
- ¬øCu√°l es el rango probable del verdadero rendimiento?
- ¬øLos intervalos se superponen entre modelos?

**4.4 Pruebas de significancia**

- Compara estad√≠sticamente tus dos mejores modelos
- ¬øLa diferencia en rendimiento es estad√≠sticamente significativa?
- ¬øO podr√≠an ser iguales de buenos?

**4.5 An√°lisis de estabilidad**

- Repite la validaci√≥n cruzada 10 veces con diferentes semillas aleatorias
- ¬øQu√© tan estables son los resultados?
- ¬øAlg√∫n modelo es m√°s consistente que otros?

### Entregables

- Validaci√≥n cruzada implementada con m√∫ltiples m√©tricas
- Intervalos de confianza calculados
- An√°lisis estad√≠stico de diferencias entre modelos
- Evaluaci√≥n de estabilidad y consistencia

---

## Ejercicio Bonus: Reporte Profesional de Evaluaci√≥n (30 minutos)

### Contexto

Como cient√≠fico de datos profesional, necesitas comunicar la calidad de tus modelos de manera clara y convincente a audiencias no t√©cnicas.

### Tareas

**Bonus.1 Dashboard de m√©tricas**

- Crea un dashboard visual que muestre todas las m√©tricas importantes
- Incluye comparaciones entre modelos
- Hazlo comprensible para directivos deportivos

**Bonus.2 Reporte ejecutivo**

Escribe un reporte de 1 p√°gina que incluya:

- **Resumen ejecutivo:** ¬øCu√°l es el mejor modelo y por qu√©?
- **M√©tricas clave:** Las 3 m√©tricas m√°s importantes
- **Confianza:** ¬øQu√© tan confiables son las predicciones?
- **Recomendaciones:** ¬øC√≥mo usar el modelo en la pr√°ctica?
- **Limitaciones:** ¬øQu√© no puede hacer el modelo?

**Bonus.3 An√°lisis de casos de uso**

- **Caso 1:** Apostar en partidos - ¬øQu√© m√©tricas importan m√°s?
- **Caso 2:** Planificaci√≥n de equipo - ¬øQu√© tipo de errores son aceptables?
- **Caso 3:** An√°lisis medi√°tico - ¬øQu√© nivel de confianza se necesita?

### Entregables

- Dashboard visual profesional
- Reporte ejecutivo de 1 p√°gina
- An√°lisis espec√≠fico por casos de uso

---

## Criterios de Evaluaci√≥n

### Competencia T√©cnica (40%)

- **C√°lculo correcto:** Todas las m√©tricas calculadas apropiadamente
- **Implementaci√≥n:** Validaci√≥n cruzada y an√°lisis estad√≠stico correctos
- **Visualizaciones:** Gr√°ficos claros y informativos

### An√°lisis e Interpretaci√≥n (40%)

- **Comprensi√≥n profunda:** Interpretaci√≥n correcta de cada m√©trica
- **Pensamiento cr√≠tico:** Identificaci√≥n de fortalezas y limitaciones
- **Contexto deportivo:** Aplicaci√≥n apropiada al dominio futbol√≠stico

### Comunicaci√≥n Profesional (20%)

- **Claridad:** Explicaciones comprensibles para audiencias no t√©cnicas
- **Recomendaciones:** Sugerencias pr√°cticas y actionables
- **Presentaci√≥n:** Organizaci√≥n profesional de resultados

## Recursos de Apoyo

### Documentaci√≥n T√©cnica

- [Scikit-learn Metrics Guide](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [ROC Curves Explained](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)
- [Cross-validation Guide](https://scikit-learn.org/stable/modules/cross_validation.html)

### Conceptos Clave

- **Precision vs Recall:** Trade-off entre exactitud y completitud
- **ROC-AUC:** Capacidad discriminativa del modelo
- **Cross-validation:** Validaci√≥n robusta de rendimiento
- **Statistical Significance:** Confianza en las diferencias observadas

## Entrega

- **Formato:** Jupyter Notebook (.ipynb) + Reporte PDF (1 p√°gina)
- **Nombre:** `apellido_nombre_semana13_evaluacion_modelos.ipynb`
- **Fecha l√≠mite:** [Definir seg√∫n cronograma]
- **Plataforma:** [Especificar LMS o sistema de entrega]

---

*¬°Excelente! Ya sabes evaluar modelos como un profesional. En la pr√≥xima semana aprenderemos a mejorar nuestros modelos mediante feature engineering y optimizaci√≥n avanzada.*

**Tiempo total estimado:** 3-4 horas  
**Dificultad:** ‚≠ê‚≠ê‚≠ê‚≠ê (Alto)  
**Prerrequisitos:** Completar Ejercicios Semanas 11-12
